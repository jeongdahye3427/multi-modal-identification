{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from deepface.extendedmodels import Age, Gender, Race, Emotion\n",
    "from deepface import DeepFace\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((229557, 128),\n",
       " (229557, 193),\n",
       " (229557, 3),\n",
       " (229557, 10),\n",
       " (7256, 128),\n",
       " (7256, 193),\n",
       " (7256, 3),\n",
       " (7256, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading final features & embedding  \n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# load face embeddings\n",
    "facedata = np.load('./Data/Face/FV_10originalFacenet.npz')\n",
    "facenet_train, facenet_test = facedata['arr_0'], facedata['arr_1']\n",
    "# normalize input vectors\n",
    "input_encoder = Normalizer(norm='l2')\n",
    "facenet_train = input_encoder.fit_transform(facenet_train)\n",
    "facenet_test = input_encoder.transform(facenet_test)\n",
    "# label encode targets\n",
    "trainy = image_train.label.values\n",
    "testy = image_test.label.values\n",
    "\n",
    "output_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "trainy = trainy.reshape(-1,1)\n",
    "testy = testy.reshape(-1,1)\n",
    "\n",
    "trainy = output_encoder.fit_transform(trainy)\n",
    "testy = output_encoder.transform(testy)\n",
    "#load voice embeddings\n",
    "voicedata = np.load('./Data/Voice/FV_10originalvoice.npz')\n",
    "voice_train, voice_test = voicedata['arr_0'], voicedata['arr_1']\n",
    "\n",
    "# load demo features\n",
    "demodata = np.load('./Data/Demography/FV_10originaldemo.npz')\n",
    "demo_train, demo_test = demodata['arr_0'], demodata['arr_1']\n",
    "\n",
    "facenet_train.shape, voice_train.shape, demo_train.shape, trainy.shape, facenet_test.shape, voice_test.shape,demo_test.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from deepface.extendedmodels import Age, Gender, Race, Emotion\n",
    "from deepface import DeepFace\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1 # set this externally to the number of items in dataset for ideal results\n",
    "NUM_CLASSES = 10 ####change your classes num\n",
    "BATCH_SIZE = 4\n",
    "LEARN_RATE = 0.01 * (BATCH_SIZE / 128)\n",
    "MOMENTUM = 0.9\n",
    "EPOCHS = 20\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Reshape, Dense, Embedding, Dropout, LSTM, Lambda, Concatenate, \\\n",
    "    Multiply, RepeatVector, Permute, Flatten, Activation\n",
    "\n",
    "def Multimodal(num_classes=NUM_CLASSES, learn_rate=LEARN_RATE, momentum=MOMENTUM):\n",
    "    \n",
    "\n",
    "    inputs_facefeature = Input(shape=(128,))\n",
    "    inputs_voice = Input(shape=(193,))\n",
    "    inputs_demo = Input(shape=(3,))\n",
    "    \n",
    "### image\n",
    "    wt_init = keras.initializers.RandomNormal(mean=0, stddev=0.01)\n",
    "    bias_init = keras.initializers.Constant(value=0.5)\n",
    "    \n",
    "    def dense_layer(**args):\n",
    "        return keras.layers.Dense(**args, \n",
    "            kernel_initializer=wt_init, \n",
    "            bias_initializer=bias_init)\n",
    "    \n",
    "### voice\n",
    "    V1 = Dense(193, activation='relu')(inputs_voice)\n",
    "    D1 = keras.layers.Dropout(rate=0.1, name='D1')(V1)\n",
    "    V2 = Dense(128, activation='relu')(D1)\n",
    "    D2 = keras.layers.Dropout(rate=0.25, name='D2')(V2)\n",
    "    V3 = Dense(128, activation='relu')(D1)\n",
    "    D3 = keras.layers.Dropout(rate=0.5, name='D3')(V3)\n",
    "\n",
    "### demo    \n",
    "#     # demoFeature layer 추가\n",
    "    demoFeature = Dense(embedding_size, activation='tanh')(inputs_demo)\n",
    "    \n",
    "    # demoFeature 랑 face feature similarity 계산\n",
    "    sim = Multiply()([inputs_facefeature, demoFeature])\n",
    "    att = Dense(1, activation='tanh')(sim)\n",
    "    att = Flatten()(att)\n",
    "    \n",
    "    # compute the weights of eache demographic feature\n",
    "    attention = Activation('softmax')(att)\n",
    "    attention = RepeatVector(embedding_size)(attention) #embedding_size\n",
    "    \n",
    "    # similarity랑 weights 곱하기(weighted sum)\n",
    "    influence = Multiply()([sim, attention])\n",
    "    influence = Lambda(lambda x: K.sum(x, axis=1))(influence)\n",
    "    \n",
    "    # the most important demographic feature of classification face\n",
    "    influence = Dense(embedding_size)(influence)\n",
    "    \n",
    "    h = Concatenate()([inputs_facefeature, D3, influence])\n",
    "\n",
    "    Softmax = dense_layer(units=num_classes, activation=keras.activations.softmax, name='F8')(h)\n",
    "\n",
    "    model = Model(inputs=[inputs_facefeature, inputs_voice, inputs_demo],\n",
    "                      outputs=[Softmax])\n",
    "\n",
    "    sgd_opt = keras.optimizers.SGD(learning_rate=LEARN_RATE, momentum=MOMENTUM)\n",
    "    cce_loss = keras.losses.categorical_crossentropy\n",
    "    \n",
    "    model.compile(optimizer=sgd_opt, loss=cce_loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57389/57389 [==============================] - 48s 839us/step - loss: 0.6590 - accuracy: 0.7982\n",
      "Epoch 2/20\n",
      "57389/57389 [==============================] - 48s 838us/step - loss: 0.2804 - accuracy: 0.9110\n",
      "Epoch 3/20\n",
      "57389/57389 [==============================] - 48s 839us/step - loss: 0.2417 - accuracy: 0.9252\n",
      "Epoch 4/20\n",
      "57389/57389 [==============================] - 48s 839us/step - loss: 0.1974 - accuracy: 0.9423\n",
      "Epoch 5/20\n",
      "57389/57389 [==============================] - 48s 837us/step - loss: 0.2051 - accuracy: 0.9397\n",
      "Epoch 6/20\n",
      "57389/57389 [==============================] - 48s 838us/step - loss: 0.2050 - accuracy: 0.9401\n",
      "Epoch 7/20\n",
      "57389/57389 [==============================] - 48s 837us/step - loss: 0.2280 - accuracy: 0.9289\n",
      "Epoch 8/20\n",
      "57389/57389 [==============================] - 48s 837us/step - loss: 0.2363 - accuracy: 0.9264\n",
      "Epoch 9/20\n",
      "57389/57389 [==============================] - 48s 839us/step - loss: 0.2084 - accuracy: 0.9403\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ac059efd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Multimodal()\n",
    "\n",
    "es = EarlyStopping(monitor='loss', verbose=1, min_delta=0, mode='auto', patience= 5)\n",
    "mc = ModelCheckpoint('./10originaldemo_FV.h5', monitor='loss', save_best_only=True)\n",
    "\n",
    "### face, voice\n",
    "# model1.fit([facenet_train,voice_train],trainy, steps_per_epoch = facenet_train.shape[0] // BATCH_SIZE, \\\n",
    "#            epochs = EPOCHS, callbacks = [es,mc])\n",
    "\n",
    "#### face, voice, demo\n",
    "model1.fit([facenet_train,voice_train, demo_train],trainy, steps_per_epoch = facenet_train.shape[0] // BATCH_SIZE, \\\n",
    "           epochs = EPOCHS, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 0s 721us/step - loss: 0.1513 - accuracy: 0.9624\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "savedmodel = load_model('./10originaldemo_FV.h5')\n",
    "\n",
    "### face, voice\n",
    "# score = savedmodel.evaluate([facenet_test,voice_test],testy)\n",
    "\n",
    "### face, voice, demo\n",
    "score = savedmodel.evaluate([facenet_test, voice_test, demo_test],testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_authenticate_py36",
   "language": "python",
   "name": "gpu_authenticate_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
